# Captioning Photos with Generative AI

This project is part of the "Building Generative AI-Powered Applications with Python" course by IBM on Coursera.

## Project Overview

This project explores using the BLIP (Bootstrapping Language-Image Pre-training) model from the Hugging Face `transformers` library to perform two key tasks:

1.  **Image Captioning:** Automatically generate descriptive text captions for a given image.
2.  **Visual Question Answering (VQA):** Answer natural language questions about the content of an image.

## Key Files

*   `BLIP_for_image_captioning.ipynb`: A Jupyter Notebook demonstrating how to generate a caption for an image.
*   `BLIP_for_Visual_Question_Answering.ipynb`: A Jupyter Notebook showing how to ask and answer questions about an image.
*   `fluid-energy-4.jpg`: The sample image used for the demonstrations.

## Technologies Used

*   Python
*   Jupyter Notebook
*   Hugging Face `transformers` library
*   PyTorch
*   Pillow (PIL)
*   Salesforce BLIP Model